{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PARAMS = yaml.load(open(\"configs/dataset.yaml\"), Loader=yaml.SafeLoader)\n",
    "PREPROCESSING_PARAMS = yaml.load(open(\"configs/preprocessing.yaml\"), Loader=yaml.SafeLoader)\n",
    "MODEL_NAME = yaml.load(open(\"configs/model.yaml\"), Loader=yaml.SafeLoader)['MODEL_NAME']\n",
    "MODEL_PARAMS = yaml.load(open(\"configs/model.yaml\"), Loader=yaml.SafeLoader)[MODEL_NAME]\n",
    "TRAINING_PARAMS = yaml.load(open(\"configs/training.yaml\"), Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =  DATASET_PARAMS['CATEGORIES']\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = os.path.join(BASE_DIR, DATASET_PARAMS['DATA_PATH'])\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.utils import get_device\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import generate_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames_df = generate_filenames_df(TRAIN_DIR, categories)\n",
    "val_filenames_df = generate_filenames_df(VAL_DIR, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import preprocess\n",
    "\n",
    "target_input_size = tuple(PREPROCESSING_PARAMS['INPUT_SIZE'])\n",
    "train_transform = preprocess(\n",
    "    target_input_size=target_input_size,\n",
    "    rotation_range=PREPROCESSING_PARAMS['ROTATION_RANGE'],\n",
    "    width_shift_range=PREPROCESSING_PARAMS['WIDTH_SHIFT_RANGE'],\n",
    "    height_shift_range=PREPROCESSING_PARAMS['HEIGHT_SHIFT_RANGE'],\n",
    "    brightness_range=PREPROCESSING_PARAMS['BRIGHTNESS_RANGE'],\n",
    "    zoom_range=PREPROCESSING_PARAMS['ZOOM_RANGE'],\n",
    "    horizontal_flip=PREPROCESSING_PARAMS['HORIZONTAL_FLIP'],\n",
    "    vertical_flip=PREPROCESSING_PARAMS['VERTICAL_FLIP'],\n",
    "    channel_shift_range=PREPROCESSING_PARAMS['CHANNEL_SHIFT_RANGE'],\n",
    "    fill_mode=PREPROCESSING_PARAMS['FILL_MODE'],\n",
    "    shear_range=PREPROCESSING_PARAMS['SHEAR_RANGE']\n",
    "    )\n",
    "\n",
    "val_transform = preprocess(target_input_size=target_input_size) # only rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1160 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path class\n",
       "0     c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     1\n",
       "1     c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     1\n",
       "2     c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     1\n",
       "3     c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     1\n",
       "4     c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     1\n",
       "...                                                 ...   ...\n",
       "1155  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1156  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1157  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1158  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1159  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "\n",
       "[1160 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train_filenames_df =train_filenames_df.reset_index(drop=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path class\n",
       "1000  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     2\n",
       "1001  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1002  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     2\n",
       "1003  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     2\n",
       "1004  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "...                                                 ...   ...\n",
       "1155  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1156  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1157  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1158  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "1159  c:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project...     3\n",
       "\n",
       "[160 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train_filenames_df.iloc[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_train_filenames_df[test_train_filenames_df['class'] == '1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loader import Loader\n",
    "train_loader = Loader(train_filenames_df, \n",
    "                     batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                     num_workers=TRAINING_PARAMS['NUM_WORKERS'], \n",
    "                     transform=train_transform, \n",
    "                     shuffle=TRAINING_PARAMS['SHUFFLE'])\n",
    "\n",
    "val_loader = Loader(val_filenames_df, \n",
    "                    batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                    num_workers=TRAINING_PARAMS['NUM_WORKERS'], \n",
    "                    transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a sample batch from train_loader\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# # Get first batch\n",
    "# images, labels = next(iter(train_loader))\n",
    "\n",
    "# # Select first image and label from batch\n",
    "# sample_image = images[0]\n",
    "# sample_label = labels[0]\n",
    "\n",
    "# # Convert tensor to numpy for displaying\n",
    "# # Move channel dimension from [C,H,W] to [H,W,C] for plotting\n",
    "# sample_image_np = sample_image.permute(1,2,0).numpy()\n",
    "\n",
    "# # Multiply by 255 to restore original pixel values\n",
    "# sample_image_np = sample_image_np * 255\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.imshow(sample_image_np)\n",
    "# plt.title(f'Label: {sample_label}')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientCapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"CAPSNET\":\n",
    "    from src.model import EfficientCapsNet\n",
    "    from src.loss import MarginLoss, marginLoss\n",
    "\n",
    "    model = EfficientCapsNet(input_size=(MODEL_PARAMS['INPUT_SIZE']))\n",
    "    loss = MarginLoss()\n",
    "    # loss = marginLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"DENSENET121\":\n",
    "    from src.densenet import DenseNet121\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "\n",
    "    model = DenseNet121(num_classes=num_classes, dropout_rate=MODEL_PARAMS['DROPOUT_RATE'])\n",
    "    loss = CrossEntropyLoss()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=TRAINING_PARAMS['LEARNING_RATE'])\n",
    "\n",
    "# use torcheval metrics\n",
    "# metrics\n",
    "from torcheval.metrics import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassAUROC,\n",
    "    MulticlassAUPRC,\n",
    "    MulticlassRecall\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from src.metrics import (\n",
    "    MulticlassMCC,\n",
    "    MulticlassSpecificity\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"mcc\": MulticlassMCC(num_classes=num_classes, device=DEVICE),\n",
    "    \"auprc\": MulticlassAUPRC(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"auroc\": MulticlassAUROC(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"accuracy\": MulticlassAccuracy(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"f1_score\": MulticlassF1Score(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"precision\": MulticlassPrecision(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE),\n",
    "    \"recall\": MulticlassRecall(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE),\n",
    "    \"specificity\": MulticlassSpecificity(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Epoch 1/10\n",
      "Epoch 1/10, Batch 1/1160, Train Loss: 1.1663\n",
      "Epoch 1/10, Batch 2/1160, Train Loss: 1.5066\n",
      "Epoch 1/10, Batch 3/1160, Train Loss: 1.6037\n",
      "Epoch 1/10, Batch 4/1160, Train Loss: 1.6047\n",
      "Epoch 1/10, Batch 5/1160, Train Loss: 1.5992\n",
      "Epoch 1/10, Batch 6/1160, Train Loss: 1.5718\n",
      "Epoch 1/10, Batch 7/1160, Train Loss: 1.5280\n",
      "Epoch 1/10, Batch 8/1160, Train Loss: 1.4526\n",
      "Epoch 1/10, Batch 9/1160, Train Loss: 1.4254\n",
      "Epoch 1/10, Batch 10/1160, Train Loss: 1.4178\n",
      "Epoch 1/10, Batch 11/1160, Train Loss: 1.3792\n",
      "Epoch 1/10, Batch 12/1160, Train Loss: 1.3772\n",
      "Epoch 1/10, Batch 13/1160, Train Loss: 1.3547\n",
      "Epoch 1/10, Batch 14/1160, Train Loss: 1.3352\n",
      "Epoch 1/10, Batch 15/1160, Train Loss: 1.3085\n",
      "Epoch 1/10, Batch 16/1160, Train Loss: 1.2927\n",
      "Epoch 1/10, Batch 17/1160, Train Loss: 1.2835\n",
      "Epoch 1/10, Batch 18/1160, Train Loss: 1.2774\n",
      "Epoch 1/10, Batch 19/1160, Train Loss: 1.2724\n",
      "Epoch 1/10, Batch 20/1160, Train Loss: 1.2767\n",
      "Epoch 1/10, Batch 21/1160, Train Loss: 1.2810\n",
      "Epoch 1/10, Batch 22/1160, Train Loss: 1.2749\n",
      "Epoch 1/10, Batch 23/1160, Train Loss: 1.2615\n",
      "Epoch 1/10, Batch 24/1160, Train Loss: 1.2512\n",
      "Epoch 1/10, Batch 25/1160, Train Loss: 1.2412\n",
      "Epoch 1/10, Batch 26/1160, Train Loss: 1.2349\n",
      "Epoch 1/10, Batch 27/1160, Train Loss: 1.2430\n",
      "Epoch 1/10, Batch 28/1160, Train Loss: 1.2467\n",
      "Epoch 1/10, Batch 29/1160, Train Loss: 1.2437\n",
      "Epoch 1/10, Batch 30/1160, Train Loss: 1.2423\n",
      "Epoch 1/10, Batch 31/1160, Train Loss: 1.2275\n",
      "Epoch 1/10, Batch 32/1160, Train Loss: 1.2270\n",
      "Epoch 1/10, Batch 33/1160, Train Loss: 1.2131\n",
      "Epoch 1/10, Batch 34/1160, Train Loss: 1.2121\n",
      "Epoch 1/10, Batch 35/1160, Train Loss: 1.2193\n",
      "Epoch 1/10, Batch 36/1160, Train Loss: 1.2232\n",
      "Epoch 1/10, Batch 37/1160, Train Loss: 1.2401\n",
      "Epoch 1 Train - Loss: 0.0396, Metrics: {mcc: 0.1812, auprc: 0.4576, auroc: 0.6442, accuracy: 0.4636, f1_score: 0.4675, precision: 0.4733, recall: 0.4636, specificity: 0.7232}\n",
      "Epoch 1 Val - Loss: 0.0398, Metrics: {mcc: -0.0163, auprc: 0.3303, auroc: 0.4677, accuracy: 0.3265, f1_score: 0.2641, precision: 0.2492, recall: 0.3265, specificity: 0.6623}\n",
      "Model saved to artifacts/effcapsnet\\epoch_1.pth\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 2/10, Batch 1/1160, Train Loss: 0.7250\n",
      "Epoch 2/10, Batch 2/1160, Train Loss: 0.9798\n",
      "Epoch 2/10, Batch 3/1160, Train Loss: 1.0750\n",
      "Epoch 2/10, Batch 4/1160, Train Loss: 1.0832\n",
      "Epoch 2/10, Batch 5/1160, Train Loss: 1.0478\n",
      "Epoch 2/10, Batch 6/1160, Train Loss: 1.0640\n",
      "Epoch 2/10, Batch 7/1160, Train Loss: 1.0594\n",
      "Epoch 2/10, Batch 8/1160, Train Loss: 1.0565\n",
      "Epoch 2/10, Batch 9/1160, Train Loss: 1.1154\n",
      "Epoch 2/10, Batch 10/1160, Train Loss: 1.0961\n",
      "Epoch 2/10, Batch 11/1160, Train Loss: 1.0816\n",
      "Epoch 2/10, Batch 12/1160, Train Loss: 1.0693\n",
      "Epoch 2/10, Batch 13/1160, Train Loss: 1.0699\n",
      "Epoch 2/10, Batch 14/1160, Train Loss: 1.0621\n",
      "Epoch 2/10, Batch 15/1160, Train Loss: 1.0486\n",
      "Epoch 2/10, Batch 16/1160, Train Loss: 1.0480\n",
      "Epoch 2/10, Batch 17/1160, Train Loss: 1.0483\n",
      "Epoch 2/10, Batch 18/1160, Train Loss: 1.0556\n",
      "Epoch 2/10, Batch 19/1160, Train Loss: 1.0552\n",
      "Epoch 2/10, Batch 20/1160, Train Loss: 1.0441\n",
      "Epoch 2/10, Batch 21/1160, Train Loss: 1.0440\n",
      "Epoch 2/10, Batch 22/1160, Train Loss: 1.0425\n",
      "Epoch 2/10, Batch 23/1160, Train Loss: 1.0306\n",
      "Epoch 2/10, Batch 24/1160, Train Loss: 1.0311\n",
      "Epoch 2/10, Batch 25/1160, Train Loss: 1.0370\n",
      "Epoch 2/10, Batch 26/1160, Train Loss: 1.0460\n",
      "Epoch 2/10, Batch 27/1160, Train Loss: 1.0507\n",
      "Epoch 2/10, Batch 28/1160, Train Loss: 1.0538\n",
      "Epoch 2/10, Batch 29/1160, Train Loss: 1.0466\n",
      "Epoch 2/10, Batch 30/1160, Train Loss: 1.0550\n",
      "Epoch 2/10, Batch 31/1160, Train Loss: 1.0501\n",
      "Epoch 2/10, Batch 32/1160, Train Loss: 1.0473\n",
      "Epoch 2/10, Batch 33/1160, Train Loss: 1.0516\n",
      "Epoch 2/10, Batch 34/1160, Train Loss: 1.0592\n",
      "Epoch 2/10, Batch 35/1160, Train Loss: 1.0628\n",
      "Epoch 2/10, Batch 36/1160, Train Loss: 1.0661\n",
      "Epoch 2/10, Batch 37/1160, Train Loss: 1.0755\n",
      "Epoch 2 Train - Loss: 0.0343, Metrics: {mcc: 0.3122, auprc: 0.5755, auroc: 0.7288, accuracy: 0.5621, f1_score: 0.5629, precision: 0.5637, recall: 0.5621, specificity: 0.7656}\n",
      "Epoch 2 Val - Loss: 0.0452, Metrics: {mcc: 0.2545, auprc: 0.5051, auroc: 0.6720, accuracy: 0.4584, f1_score: 0.3917, precision: 0.3538, recall: 0.4584, specificity: 0.7422}\n",
      "\n",
      "Epoch 3/10\n",
      "Epoch 3/10, Batch 1/1160, Train Loss: 0.5498\n",
      "Epoch 3/10, Batch 2/1160, Train Loss: 0.9079\n",
      "Epoch 3/10, Batch 3/1160, Train Loss: 1.0120\n",
      "Epoch 3/10, Batch 4/1160, Train Loss: 1.0595\n",
      "Epoch 3/10, Batch 5/1160, Train Loss: 1.0511\n",
      "Epoch 3/10, Batch 6/1160, Train Loss: 1.0251\n",
      "Epoch 3/10, Batch 7/1160, Train Loss: 1.0069\n",
      "Epoch 3/10, Batch 8/1160, Train Loss: 1.0123\n",
      "Epoch 3/10, Batch 9/1160, Train Loss: 1.0281\n",
      "Epoch 3/10, Batch 10/1160, Train Loss: 1.0624\n",
      "Epoch 3/10, Batch 11/1160, Train Loss: 1.0385\n",
      "Epoch 3/10, Batch 12/1160, Train Loss: 1.0183\n",
      "Epoch 3/10, Batch 13/1160, Train Loss: 1.0000\n",
      "Epoch 3/10, Batch 14/1160, Train Loss: 1.0182\n",
      "Epoch 3/10, Batch 15/1160, Train Loss: 1.0117\n",
      "Epoch 3/10, Batch 16/1160, Train Loss: 1.0272\n",
      "Epoch 3/10, Batch 17/1160, Train Loss: 1.0231\n",
      "Epoch 3/10, Batch 18/1160, Train Loss: 1.0146\n",
      "Epoch 3/10, Batch 19/1160, Train Loss: 1.0211\n",
      "Epoch 3/10, Batch 20/1160, Train Loss: 1.0297\n",
      "Epoch 3/10, Batch 21/1160, Train Loss: 1.0265\n",
      "Epoch 3/10, Batch 22/1160, Train Loss: 1.0178\n",
      "Epoch 3/10, Batch 23/1160, Train Loss: 1.0160\n",
      "Epoch 3/10, Batch 24/1160, Train Loss: 1.0357\n",
      "Epoch 3/10, Batch 25/1160, Train Loss: 1.0373\n",
      "Epoch 3/10, Batch 26/1160, Train Loss: 1.0306\n",
      "Epoch 3/10, Batch 27/1160, Train Loss: 1.0310\n",
      "Epoch 3/10, Batch 28/1160, Train Loss: 1.0492\n",
      "Epoch 3/10, Batch 29/1160, Train Loss: 1.0428\n",
      "Epoch 3/10, Batch 30/1160, Train Loss: 1.0406\n",
      "Epoch 3/10, Batch 31/1160, Train Loss: 1.0312\n",
      "Epoch 3/10, Batch 32/1160, Train Loss: 1.0325\n",
      "Epoch 3/10, Batch 33/1160, Train Loss: 1.0394\n",
      "Epoch 3/10, Batch 34/1160, Train Loss: 1.0500\n",
      "Epoch 3/10, Batch 35/1160, Train Loss: 1.0539\n",
      "Epoch 3/10, Batch 36/1160, Train Loss: 1.0558\n",
      "Epoch 3/10, Batch 37/1160, Train Loss: 1.0560\n",
      "Epoch 3 Train - Loss: 0.0337, Metrics: {mcc: 0.3491, auprc: 0.6048, auroc: 0.7530, accuracy: 0.5795, f1_score: 0.5798, precision: 0.5800, recall: 0.5795, specificity: 0.7795}\n",
      "Epoch 3 Val - Loss: 0.0509, Metrics: {mcc: 0.2637, auprc: 0.5948, auroc: 0.7448, accuracy: 0.4473, f1_score: 0.4009, precision: 0.6272, recall: 0.4473, specificity: 0.7316}\n",
      "\n",
      "Epoch 4/10\n",
      "Epoch 4/10, Batch 1/1160, Train Loss: 0.8771\n",
      "Epoch 4/10, Batch 2/1160, Train Loss: 1.0047\n",
      "Epoch 4/10, Batch 3/1160, Train Loss: 0.8836\n",
      "Epoch 4/10, Batch 4/1160, Train Loss: 0.9659\n",
      "Epoch 4/10, Batch 5/1160, Train Loss: 0.9939\n",
      "Epoch 4/10, Batch 6/1160, Train Loss: 0.9896\n",
      "Epoch 4/10, Batch 7/1160, Train Loss: 0.9527\n",
      "Epoch 4/10, Batch 8/1160, Train Loss: 0.9433\n",
      "Epoch 4/10, Batch 9/1160, Train Loss: 0.9561\n",
      "Epoch 4/10, Batch 10/1160, Train Loss: 0.9751\n",
      "Epoch 4/10, Batch 11/1160, Train Loss: 0.9517\n",
      "Epoch 4/10, Batch 12/1160, Train Loss: 0.9300\n",
      "Epoch 4/10, Batch 13/1160, Train Loss: 0.9214\n",
      "Epoch 4/10, Batch 14/1160, Train Loss: 0.9269\n",
      "Epoch 4/10, Batch 15/1160, Train Loss: 0.9233\n",
      "Epoch 4/10, Batch 16/1160, Train Loss: 0.9443\n",
      "Epoch 4/10, Batch 17/1160, Train Loss: 0.9343\n",
      "Epoch 4/10, Batch 18/1160, Train Loss: 0.9376\n",
      "Epoch 4/10, Batch 19/1160, Train Loss: 0.9226\n",
      "Epoch 4/10, Batch 20/1160, Train Loss: 0.9276\n",
      "Epoch 4/10, Batch 21/1160, Train Loss: 0.9341\n",
      "Epoch 4/10, Batch 22/1160, Train Loss: 0.9384\n",
      "Epoch 4/10, Batch 23/1160, Train Loss: 0.9292\n",
      "Epoch 4/10, Batch 24/1160, Train Loss: 0.9383\n",
      "Epoch 4/10, Batch 25/1160, Train Loss: 0.9508\n",
      "Epoch 4/10, Batch 26/1160, Train Loss: 0.9380\n",
      "Epoch 4/10, Batch 27/1160, Train Loss: 0.9401\n",
      "Epoch 4/10, Batch 28/1160, Train Loss: 0.9463\n",
      "Epoch 4/10, Batch 29/1160, Train Loss: 0.9338\n",
      "Epoch 4/10, Batch 30/1160, Train Loss: 0.9313\n",
      "Epoch 4/10, Batch 31/1160, Train Loss: 0.9335\n",
      "Epoch 4/10, Batch 32/1160, Train Loss: 0.9339\n",
      "Epoch 4/10, Batch 33/1160, Train Loss: 0.9313\n",
      "Epoch 4/10, Batch 34/1160, Train Loss: 0.9438\n",
      "Epoch 4/10, Batch 35/1160, Train Loss: 0.9479\n",
      "Epoch 4/10, Batch 36/1160, Train Loss: 0.9496\n",
      "Epoch 4/10, Batch 37/1160, Train Loss: 0.9611\n",
      "Epoch 4 Train - Loss: 0.0307, Metrics: {mcc: 0.3945, auprc: 0.6301, auroc: 0.7762, accuracy: 0.6072, f1_score: 0.6106, precision: 0.6145, recall: 0.6072, specificity: 0.7938}\n",
      "Epoch 4 Val - Loss: 0.0426, Metrics: {mcc: 0.3085, auprc: 0.6002, auroc: 0.7479, accuracy: 0.4928, f1_score: 0.4798, precision: 0.6095, recall: 0.4928, specificity: 0.7534}\n",
      "\n",
      "Epoch 5/10\n",
      "Epoch 5/10, Batch 1/1160, Train Loss: 0.6418\n",
      "Epoch 5/10, Batch 2/1160, Train Loss: 0.8156\n",
      "Epoch 5/10, Batch 3/1160, Train Loss: 0.8286\n",
      "Epoch 5/10, Batch 4/1160, Train Loss: 0.8353\n",
      "Epoch 5/10, Batch 5/1160, Train Loss: 0.8593\n",
      "Epoch 5/10, Batch 6/1160, Train Loss: 0.8407\n",
      "Epoch 5/10, Batch 7/1160, Train Loss: 0.8332\n",
      "Epoch 5/10, Batch 8/1160, Train Loss: 0.8377\n",
      "Epoch 5/10, Batch 9/1160, Train Loss: 0.8399\n",
      "Epoch 5/10, Batch 10/1160, Train Loss: 0.8948\n",
      "Epoch 5/10, Batch 11/1160, Train Loss: 0.8741\n",
      "Epoch 5/10, Batch 12/1160, Train Loss: 0.8765\n",
      "Epoch 5/10, Batch 13/1160, Train Loss: 0.9186\n",
      "Epoch 5/10, Batch 14/1160, Train Loss: 0.9211\n",
      "Epoch 5/10, Batch 15/1160, Train Loss: 0.9200\n",
      "Epoch 5/10, Batch 16/1160, Train Loss: 0.9072\n",
      "Epoch 5/10, Batch 17/1160, Train Loss: 0.8903\n",
      "Epoch 5/10, Batch 18/1160, Train Loss: 0.8983\n",
      "Epoch 5/10, Batch 19/1160, Train Loss: 0.8875\n",
      "Epoch 5/10, Batch 20/1160, Train Loss: 0.8985\n",
      "Epoch 5/10, Batch 21/1160, Train Loss: 0.8920\n",
      "Epoch 5/10, Batch 22/1160, Train Loss: 0.8885\n",
      "Epoch 5/10, Batch 23/1160, Train Loss: 0.8780\n",
      "Epoch 5/10, Batch 24/1160, Train Loss: 0.8825\n",
      "Epoch 5/10, Batch 25/1160, Train Loss: 0.8867\n",
      "Epoch 5/10, Batch 26/1160, Train Loss: 0.8988\n",
      "Epoch 5/10, Batch 27/1160, Train Loss: 0.9001\n",
      "Epoch 5/10, Batch 28/1160, Train Loss: 0.9086\n",
      "Epoch 5/10, Batch 29/1160, Train Loss: 0.8969\n",
      "Epoch 5/10, Batch 30/1160, Train Loss: 0.8966\n",
      "Epoch 5/10, Batch 31/1160, Train Loss: 0.8971\n",
      "Epoch 5/10, Batch 32/1160, Train Loss: 0.9073\n",
      "Epoch 5/10, Batch 33/1160, Train Loss: 0.9082\n",
      "Epoch 5/10, Batch 34/1160, Train Loss: 0.9108\n",
      "Epoch 5/10, Batch 35/1160, Train Loss: 0.9168\n",
      "Epoch 5/10, Batch 36/1160, Train Loss: 0.9259\n",
      "Epoch 5/10, Batch 37/1160, Train Loss: 0.9266\n",
      "Epoch 5 Train - Loss: 0.0296, Metrics: {mcc: 0.4184, auprc: 0.6573, auroc: 0.7864, accuracy: 0.6299, f1_score: 0.6310, precision: 0.6322, recall: 0.6299, specificity: 0.8010}\n",
      "Epoch 5 Val - Loss: 0.0457, Metrics: {mcc: 0.2964, auprc: 0.6229, auroc: 0.7454, accuracy: 0.5017, f1_score: 0.4987, precision: 0.5794, recall: 0.5017, specificity: 0.7562}\n",
      "\n",
      "Epoch 6/10\n",
      "Epoch 6/10, Batch 1/1160, Train Loss: 0.9817\n",
      "Epoch 6/10, Batch 2/1160, Train Loss: 0.8583\n",
      "Epoch 6/10, Batch 3/1160, Train Loss: 0.8066\n",
      "Epoch 6/10, Batch 4/1160, Train Loss: 0.8360\n",
      "Epoch 6/10, Batch 5/1160, Train Loss: 0.8733\n",
      "Epoch 6/10, Batch 6/1160, Train Loss: 0.8345\n",
      "Epoch 6/10, Batch 7/1160, Train Loss: 0.8990\n",
      "Epoch 6/10, Batch 8/1160, Train Loss: 0.8897\n",
      "Epoch 6/10, Batch 9/1160, Train Loss: 0.9228\n",
      "Epoch 6/10, Batch 10/1160, Train Loss: 0.9448\n",
      "Epoch 6/10, Batch 11/1160, Train Loss: 0.9327\n",
      "Epoch 6/10, Batch 12/1160, Train Loss: 0.9240\n",
      "Epoch 6/10, Batch 13/1160, Train Loss: 0.9272\n",
      "Epoch 6/10, Batch 14/1160, Train Loss: 0.9566\n",
      "Epoch 6/10, Batch 15/1160, Train Loss: 0.9598\n",
      "Epoch 6/10, Batch 16/1160, Train Loss: 0.9532\n",
      "Epoch 6/10, Batch 17/1160, Train Loss: 0.9581\n",
      "Epoch 6/10, Batch 18/1160, Train Loss: 0.9649\n",
      "Epoch 6/10, Batch 19/1160, Train Loss: 0.9616\n",
      "Epoch 6/10, Batch 20/1160, Train Loss: 0.9729\n",
      "Epoch 6/10, Batch 21/1160, Train Loss: 0.9786\n",
      "Epoch 6/10, Batch 22/1160, Train Loss: 0.9731\n",
      "Epoch 6/10, Batch 23/1160, Train Loss: 0.9707\n",
      "Epoch 6/10, Batch 24/1160, Train Loss: 0.9868\n",
      "Epoch 6/10, Batch 25/1160, Train Loss: 0.9846\n",
      "Epoch 6/10, Batch 26/1160, Train Loss: 0.9858\n",
      "Epoch 6/10, Batch 27/1160, Train Loss: 0.9703\n",
      "Epoch 6/10, Batch 28/1160, Train Loss: 0.9640\n",
      "Epoch 6/10, Batch 29/1160, Train Loss: 0.9592\n",
      "Epoch 6/10, Batch 30/1160, Train Loss: 0.9478\n",
      "Epoch 6/10, Batch 31/1160, Train Loss: 0.9436\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNUM_EPOCHS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRINT_EVERY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAVE_PATIENCE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAVE_PATH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAVE_MODEL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAVE_METRICS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\Documents\\Kuliah Sarjana\\Project\\XDL_Collitis\\src\\train.py:80\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, metrics, print_every, save_patience, save_path, save_model, save_metrics)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!!! NaN or Inf detected in loss !!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\xdl-env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\xdl-env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\xdl-env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from src.train import train\n",
    "\n",
    "history = train(model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    criterion=loss, \n",
    "    optimizer=optimizer, \n",
    "    num_epochs=TRAINING_PARAMS['NUM_EPOCHS'], \n",
    "    device=DEVICE,\n",
    "    metrics=metrics,\n",
    "    print_every=TRAINING_PARAMS['PRINT_EVERY'],\n",
    "    save_patience=TRAINING_PARAMS['SAVE_PATIENCE'],\n",
    "    save_path=TRAINING_PARAMS['SAVE_PATH'],\n",
    "    save_model=TRAINING_PARAMS['SAVE_MODEL'],\n",
    "    save_metrics=TRAINING_PARAMS['SAVE_METRICS']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xdl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
